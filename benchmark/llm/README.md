# LLM/MLLM benchmark

## LLM

### Task benchmark

- **Chat**: Text-based conversational AI applications, like ChatGPT and Claude.
- **Code completion**: Fill-in-the-middle/infilling code generation, like Copilot and Cursor (without agent mode).
- **Problem-solving with reasoning**: Deriving the answer to a well-defined and challenging question with reasoning.
- **Agent (web search)**: An LLM augmented with a web search tool, like Perplexity and ChatGPT Deep Research.
- **Agent (coding)**: Complex programming tasks, like Cursor (agent mode), OpenAI Codex, and Claude Code.


## MLLM

A Multimodal LLM (MLLM) can pretty much do everything a regular LLM can do, with the addition of handling multimodal inputs.
We benchmark MLLM as a separate category to understand the implications of handling multimodal inputs.

### Task benchmark

- **Multimodal chat**: Conversational AI applications that can handle text, image, video, and audio inputs.
   - Image chat
   - Video chat
   - Audio chat
