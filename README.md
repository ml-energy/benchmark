# The ML.ENERGY Benchmark

[![Leaderboard](https://custom-icon-badges.demolab.com/badge/ml.energy-leaderboard-23d175.svg?logo=home&logoColor=white&logoSource=feather)](https://ml.energy/leaderboard)
[![Paper](https://custom-icon-badges.herokuapp.com/badge/NeurIPS'25%20D%26B-Spotlight-b31b1b.svg)](https://arxiv.org/abs/2505.06371)
[![Apache-2.0 License](https://custom-icon-badges.herokuapp.com/github/license/ml-energy/leaderboard?logo=law)](/LICENSE)

Benchmarking framework for measuring energy consumption and performance of generative AI models like Large Language Models (LLMs), Multimodal LLMs (MLLMs), and Diffusion models.

You can browse [The ML.ENERGY Leaderboard](https://ml.energy/leaderboard) for the latest benchmarking results.

- **[Overview](docs/overview.md)**: Tasks, datasets, runtime
- **[Data Preparation](docs/data-preparation.md)**: Downloading necessary datasets and processing them
- **[Running Benchmarks](docs/running-benchmarks.md)**: Job generation and manual execution
- **[Analyzing Results](docs/analyzing-results.md)**: Analyzing and understanding benchmarking results


## Citation

```bibtex
@inproceedings{mlenergy-neuripsdb25,
    title={The {ML.ENERGY Benchmark}: Toward Automated Inference Energy Measurement and Optimization}, 
    author={Jae-Won Chung and Jeff J. Ma and Ruofan Wu and Jiachen Liu and Oh Jun Kweon and Yuxuan Xia and Zhiyu Wu and Mosharaf Chowdhury},
    year={2025},
    booktitle={NeurIPS Datasets and Benchmarks},
}
```
