command_template: |
  python -m mlenergy.llm.benchmark \
    --container-runtime {container_runtime} \
    --server-image {server_image} \
    --max-output-tokens {max_output_tokens} \
    workload:video-chat \
    --workload.base-dir {run_dir}/mllm \
    --workload.model-id {model_id} \
    --workload.num-requests {num_requests} \
    --workload.num-request-repeats {num_request_repeats} \
    --workload.num-videos {num_videos} \
    --workload.video-data-dir ${{VIDEO_DATA_DIR:?VIDEO_DATA_DIR not set}} \
    --workload.gpu-model {gpu_model} \
    --workload.num-gpus {num_gpus} \
    --workload.max-num-seqs {max_num_seqs}

workload_defaults:
  max_output_tokens: 4096
  run_dir: run
  num_requests: 512
  num_request_repeats: 1
  num_videos: 1

sweep_defaults:
  - max_num_seqs: [8, 16, 32, 64, 96, 128, 192, 256, 384, 512]
